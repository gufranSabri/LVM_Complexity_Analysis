Model: vit_base_patch16_224
Dataset size: 50000
FLOPs per instance: 16866721536
Average time per forward pass: 0.0032319330811843546
Training time per epoch: 161.75178684711457
Per instance inference latency: 8.156160354614258
Per batch GPU memory consumption (batch size 4): 1361
Per batch GPU memory consumption (batch size 8): 1391
Per batch GPU memory consumption (batch size 16): 1451
Per batch GPU memory consumption (batch size 32): 1572
Per batch GPU memory consumption (batch size 64): 1812
Per batch GPU memory consumption (batch size 128): 2293
Per batch GPU memory consumption (batch size 256): 3254
Validation Loss: 2.2999
Validation Accuracy: 52.20%
